#!/usr/bin/env python3
"""
Dataset Generator for File System MCP Tool Training
‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≥‡πÄ‡∏ô‡∏¥‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å AI Agent ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå
"""

import json
import random
import sqlite3
from datetime import datetime
from typing import Dict, List, Tuple, Any
from pathlib import Path
import os

class FileSystemDatasetGenerator:
    """‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏Å‡∏≥‡πÄ‡∏ô‡∏¥‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å AI Agent"""
    
    def __init__(self, db_path: str = "file_system_analysis.db"):
        self.db_path = db_path
        self.dataset = []
        
    def generate_training_dataset(self, output_file: str = "file_system_training_dataset.json"):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å AI Agent"""
        print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å AI Agent...")
        
        # 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        self._generate_basic_queries()
        
        # 2. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå
        self._generate_file_search_queries()
        
        # 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        self._generate_analysis_queries()
        
        # 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SQL queries
        self._generate_sql_queries()
        
        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå
        self._generate_file_management_queries()
        
        # 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
        self._generate_reporting_queries()
        
        # 7. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        self._save_dataset(output_file)
        
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(self.dataset)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        print(f"üìÅ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏õ‡∏¢‡∏±‡∏á: {output_file}")
        
    def _generate_basic_queries(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô"""
        basic_queries = [
            {
                "instruction": "‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏ó‡∏µ",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_directory_summary",
                    "session_id": "scan_xxx",
                    "args": []
                },
                "category": "basic",
                "description": "‡∏Ç‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå"
            },
            {
                "instruction": "‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Å‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT COUNT(*) as total_files FROM files WHERE session_id = ?",
                    "params": ["scan_xxx"]
                },
                "category": "basic",
                "description": "‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"
            },
            {
                "instruction": "‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT SUM(file_size) as total_size FROM files WHERE session_id = ?",
                    "params": ["scan_xxx"]
                },
                "category": "basic",
                "description": "‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå"
            },
            {
                "instruction": "‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 10 ‡πÑ‡∏ü‡∏•‡πå",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_largest_files",
                    "session_id": "scan_xxx",
                    "args": [10]
                },
                "category": "basic",
                "description": "‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
            },
            {
                "instruction": "‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÑ‡∏´‡∏°",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_duplicate_files",
                    "session_id": "scan_xxx",
                    "args": []
                },
                "category": "basic",
                "description": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥"
            }
        ]
        
        self.dataset.extend(basic_queries)
        
    def _generate_file_search_queries(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå"""
        file_search_queries = [
            {
                "instruction": "‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ Word ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 5 ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡∏´‡∏ô‡πà‡∏≠‡∏¢",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND file_extension = '.docx' ORDER BY file_size DESC LIMIT 5",
                    "params": ["scan_xxx"]
                },
                "category": "file_search",
                "description": "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå Word ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà"
            },
            {
                "instruction": "‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND mime_type LIKE 'image/%'",
                    "params": ["scan_xxx"]
                },
                "category": "file_search",
                "description": "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û"
            },
            {
                "instruction": "‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå PDF ‡∏Å‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT COUNT(*) as pdf_count FROM files WHERE session_id = ? AND file_extension = '.pdf'",
                    "params": ["scan_xxx"]
                },
                "category": "file_search",
                "description": "‡∏ô‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå PDF"
            },
            {
                "instruction": "‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ 'report' ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_path FROM files WHERE session_id = ? AND file_name LIKE '%report%' AND file_extension = '.pdf'",
                    "params": ["scan_xxx"]
                },
                "category": "file_search",
                "description": "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ report"
            },
            {
                "instruction": "‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏Ñ‡πâ‡∏î Python ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND file_extension = '.py' ORDER BY file_size DESC LIMIT 1",
                    "params": ["scan_xxx"]
                },
                "category": "file_search",
                "description": "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå Python ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
            },
            {
                "instruction": "‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå JavaScript ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î",
                "correct_action": {
                    "action": "query_function",
                    "function": "find_files_by_extension",
                    "session_id": "scan_xxx",
                    "args": [".js"]
                },
                "category": "file_search",
                "description": "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå JavaScript"
            },
            {
                "instruction": "‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏´‡∏ô‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡∏•‡∏∑‡∏≠‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_duplicate_files",
                    "session_id": "scan_xxx",
                    "args": []
                },
                "category": "file_search",
                "description": "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏∑‡∏≠‡∏á‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà"
            }
        ]
        
        self.dataset.extend(file_search_queries)
        
    def _generate_analysis_queries(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"""
        analysis_queries = [
            {
                "instruction": "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏û‡∏ö‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_extension, COUNT(*) as count, SUM(file_size) as total_size FROM files WHERE session_id = ? GROUP BY file_extension ORDER BY count DESC",
                    "params": ["scan_xxx"]
                },
                "category": "analysis",
                "description": "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå"
            },
            {
                "instruction": "‡πÑ‡∏ü‡∏•‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏´‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_extension, SUM(file_size) as total_size FROM files WHERE session_id = ? GROUP BY file_extension ORDER BY total_size DESC LIMIT 5",
                    "params": ["scan_xxx"]
                },
                "category": "analysis",
                "description": "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏Å"
            },
            {
                "instruction": "‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ã‡πà‡∏≠‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT COUNT(*) as hidden_count FROM files WHERE session_id = ? AND is_hidden = 1",
                    "params": ["scan_xxx"]
                },
                "category": "analysis",
                "description": "‡∏ô‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πà‡∏≠‡∏ô"
            },
            {
                "instruction": "‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏´‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, created_date FROM files WHERE session_id = ? ORDER BY created_date ASC LIMIT 10",
                    "params": ["scan_xxx"]
                },
                "category": "analysis",
                "description": "‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î"
            },
            {
                "instruction": "‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏´‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, modified_date FROM files WHERE session_id = ? ORDER BY modified_date DESC LIMIT 10",
                    "params": ["scan_xxx"]
                },
                "category": "analysis",
                "description": "‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"
            }
        ]
        
        self.dataset.extend(analysis_queries)
        
    def _generate_sql_queries(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• SQL queries"""
        sql_queries = [
            {
                "instruction": "‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 100MB",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND file_size > 104857600 ORDER BY file_size DESC",
                    "params": ["scan_xxx"]
                },
                "category": "sql",
                "description": "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà"
            },
            {
                "instruction": "‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 10MB ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND mime_type LIKE 'image/%' AND file_size > 10485760",
                    "params": ["scan_xxx"]
                },
                "category": "sql",
                "description": "‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà"
            },
            {
                "instruction": "‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ô‡∏µ‡πâ",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, created_date FROM files WHERE session_id = ? AND created_date >= date('now', 'start of month')",
                    "params": ["scan_xxx"]
                },
                "category": "sql",
                "description": "‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô"
            },
            {
                "instruction": "‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, modified_date FROM files WHERE session_id = ? AND modified_date >= date('now', '-7 days')",
                    "params": ["scan_xxx"]
                },
                "category": "sql",
                "description": "‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡∏ô‡∏µ‡πâ"
            },
            {
                "instruction": "‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path FROM files WHERE session_id = ? AND (file_extension = '' OR file_extension IS NULL)",
                    "params": ["scan_xxx"]
                },
                "category": "sql",
                "description": "‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ô‡∏≤‡∏°‡∏™‡∏Å‡∏∏‡∏•"
            }
        ]
        
        self.dataset.extend(sql_queries)
        
    def _generate_file_management_queries(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå"""
        management_queries = [
            {
                "instruction": "‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏´‡∏ô‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏•‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND file_size > 52428800 ORDER BY file_size DESC LIMIT 20",
                    "params": ["scan_xxx"]
                },
                "category": "management",
                "description": "‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡∏•‡∏ö"
            },
            {
                "instruction": "‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path FROM files WHERE session_id = ? AND (file_name LIKE '%.tmp' OR file_name LIKE 'temp%' OR file_name LIKE '%cache%')",
                    "params": ["scan_xxx"]
                },
                "category": "management",
                "description": "‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß"
            },
            {
                "instruction": "‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏´‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, accessed_date FROM files WHERE session_id = ? ORDER BY accessed_date DESC LIMIT 10",
                    "params": ["scan_xxx"]
                },
                "category": "management",
                "description": "‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"
            },
            {
                "instruction": "‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ß‡∏£‡∏±‡∏™",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path FROM files WHERE session_id = ? AND file_extension IN ('.exe', '.bat', '.cmd', '.scr', '.pif')",
                    "params": ["scan_xxx"]
                },
                "category": "management",
                "description": "‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ß‡∏£‡∏±‡∏™"
            }
        ]
        
        self.dataset.extend(management_queries)
        
    def _generate_reporting_queries(self):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô"""
        reporting_queries = [
            {
                "instruction": "‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_directory_summary",
                    "session_id": "scan_xxx",
                    "args": []
                },
                "category": "reporting",
                "description": "‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ"
            },
            {
                "instruction": "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 20 ‡πÑ‡∏ü‡∏•‡πå",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_largest_files",
                    "session_id": "scan_xxx",
                    "args": [20]
                },
                "category": "reporting",
                "description": "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà"
            },
            {
                "instruction": "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_duplicate_files",
                    "session_id": "scan_xxx",
                    "args": []
                },
                "category": "reporting",
                "description": "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥"
            },
            {
                "instruction": "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_extension, COUNT(*) as count, AVG(file_size) as avg_size FROM files WHERE session_id = ? GROUP BY file_extension ORDER BY count DESC",
                    "params": ["scan_xxx"]
                },
                "category": "reporting",
                "description": "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå"
            }
        ]
        
        self.dataset.extend(reporting_queries)
        
    def _save_dataset(self, output_file: str):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        dataset_info = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_samples": len(self.dataset),
                "categories": list(set(item["category"] for item in self.dataset)),
                "description": "Dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å AI Agent ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ File System MCP Tool"
            },
            "dataset": self.dataset
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, ensure_ascii=False, indent=2)
            
    def generate_variations(self, base_dataset_file: str, output_file: str = "expanded_dataset.json"):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô"""
        print("üîÑ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢...")
        
        with open(base_dataset_file, 'r', encoding='utf-8') as f:
            base_data = json.load(f)
            
        expanded_dataset = base_data["dataset"].copy()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á
        variations = [
            ("‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà", ["‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î", "‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏´‡∏ô‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î", "‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"]),
            ("‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥", ["‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥‡πÑ‡∏´‡∏°", "‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏´‡∏ô‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô", "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ã‡πâ‡∏≥"]),
            ("‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•", ["‡∏™‡∏£‡∏∏‡∏õ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå", "‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ", "‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ"]),
            ("‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û", ["‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏π‡∏õ", "‡∏£‡∏π‡∏õ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡∏ö‡πâ‡∏≤‡∏á"]),
            ("‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£", ["‡πÑ‡∏ü‡∏•‡πå Word", "‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", "‡πÑ‡∏ü‡∏•‡πå .docx"])
        ]
        
        for original, variations_list in variations:
            for variation in variations_list:
                # ‡∏´‡∏≤ entry ‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö original
                for entry in base_data["dataset"]:
                    if original in entry["instruction"]:
                        new_entry = entry.copy()
                        new_entry["instruction"] = entry["instruction"].replace(original, variation)
                        new_entry["category"] = "variation"
                        expanded_dataset.append(new_entry)
                        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ç‡∏¢‡∏≤‡∏¢‡πÅ‡∏•‡πâ‡∏ß
        expanded_info = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_samples": len(expanded_dataset),
                "base_samples": len(base_data["dataset"]),
                "variations_added": len(expanded_dataset) - len(base_data["dataset"]),
                "description": "Expanded dataset with variations"
            },
            "dataset": expanded_dataset
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(expanded_info, f, ensure_ascii=False, indent=2)
            
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏≤‡∏¢‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(expanded_dataset)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
    def generate_test_dataset(self, training_dataset_file: str, output_file: str = "test_dataset.json"):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö"""
        print("üß™ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö...")
        
        with open(training_dataset_file, 'r', encoding='utf-8') as f:
            training_data = json.load(f)
            
        # ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 20% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö
        test_samples = random.sample(training_data["dataset"], 
                                   min(len(training_data["dataset"]) // 5, 50))
        
        test_info = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_samples": len(test_samples),
                "source": training_dataset_file,
                "description": "Test dataset for File System MCP Tool"
            },
            "dataset": test_samples
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(test_info, f, ensure_ascii=False, indent=2)
            
        print(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {len(test_samples)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        
    def analyze_dataset(self, dataset_file: str):
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        print(f"üìä ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {dataset_file}")
        
        with open(dataset_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        dataset = data["dataset"]
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏≤‡∏°‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà
        categories = {}
        for item in dataset:
            cat = item["category"]
            if cat not in categories:
                categories[cat] = 0
            categories[cat] += 1
            
        print("\nüìà ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
        print(f"‚Ä¢ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(dataset)}")
        print(f"‚Ä¢ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà: {len(categories)}")
        
        print("\nüìã ‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà:")
        for cat, count in sorted(categories.items()):
            percentage = (count / len(dataset)) * 100
            print(f"  ‚Ä¢ {cat}: {count} ({percentage:.1f}%)")
            
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå actions
        actions = {}
        for item in dataset:
            action = item["correct_action"]["action"]
            if action not in actions:
                actions[action] = 0
            actions[action] += 1
            
        print("\nüîß Actions ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:")
        for action, count in sorted(actions.items()):
            percentage = (count / len(dataset)) * 100
            print(f"  ‚Ä¢ {action}: {count} ({percentage:.1f}%)")

def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    print("üöÄ File System MCP Dataset Generator")
    print("=" * 50)
    
    generator = FileSystemDatasetGenerator()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
    generator.generate_training_dataset("file_system_training_dataset.json")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏≤‡∏¢
    generator.generate_variations("file_system_training_dataset.json", "expanded_dataset.json")
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    generator.generate_test_dataset("file_system_training_dataset.json", "test_dataset.json")
    
    # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    print("\n" + "=" * 50)
    generator.analyze_dataset("file_system_training_dataset.json")
    
    print("\n" + "=" * 50)
    generator.analyze_dataset("expanded_dataset.json")
    
    print("\n" + "=" * 50)
    generator.analyze_dataset("test_dataset.json")
    
    print("\nüéâ ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô!")
    print("\nüìÅ ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á:")
    print("  ‚Ä¢ file_system_training_dataset.json - ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô")
    print("  ‚Ä¢ expanded_dataset.json - ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏¢‡∏≤‡∏¢")
    print("  ‚Ä¢ test_dataset.json - ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö")

if __name__ == "__main__":
    main()
