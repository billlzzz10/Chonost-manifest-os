"""
üöÄ Enhanced RAG System - High Performance & Deep Document Scanning
‡∏£‡∏∞‡∏ö‡∏ö RAG ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏•‡∏∂‡∏Å

Features:
- Parallel Document Processing
- Intelligent File Type Detection
- Advanced Caching Strategy
- Performance Monitoring
- Deep Code Analysis
- Document Content Extraction
"""

import asyncio
import json
import logging
import hashlib
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Union, Tuple, Set
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np
import sqlite3
import redis
import requests
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
import multiprocessing
import mimetypes
import magic
import re
import os

# Document Processing Imports
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    import docx
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    import openpyxl
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    HTML_AVAILABLE = True
except ImportError:
    HTML_AVAILABLE = False

# Code Analysis Imports
try:
    import ast
    import tokenize
    AST_AVAILABLE = True
except ImportError:
    AST_AVAILABLE = False

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô"""
    start_time: float
    end_time: float
    files_processed: int
    documents_extracted: int
    embeddings_generated: int
    cache_hits: int
    cache_misses: int
    errors: List[str]
    
    @property
    def total_time(self) -> float:
        return self.end_time - self.start_time
    
    @property
    def files_per_second(self) -> float:
        return self.files_processed / self.total_time if self.total_time > 0 else 0
    
    @property
    def documents_per_second(self) -> float:
        return self.documents_extracted / self.total_time if self.total_time > 0 else 0

@dataclass
class DocumentContent:
    """‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ"""
    file_path: str
    content_type: str  # 'text', 'code', 'document', 'image', 'binary'
    content: str
    metadata: Dict[str, Any]
    extracted_at: datetime
    file_size: int
    processing_time: float

class IntelligentFileProcessor:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.supported_extensions = {
            # Text & Code Files
            '.txt': 'text', '.md': 'text', '.rst': 'text', '.log': 'text',
            '.py': 'code', '.js': 'code', '.ts': 'code', '.jsx': 'code', '.tsx': 'code',
            '.java': 'code', '.cpp': 'code', '.c': 'code', '.h': 'code', '.hpp': 'code',
            '.cs': 'code', '.php': 'code', '.rb': 'code', '.go': 'code', '.rs': 'code',
            '.swift': 'code', '.kt': 'code', '.scala': 'code', '.dart': 'code',
            '.html': 'code', '.css': 'code', '.scss': 'code', '.sass': 'code',
            '.xml': 'code', '.json': 'code', '.yaml': 'code', '.yml': 'code',
            '.toml': 'code', '.ini': 'code', '.cfg': 'code', '.conf': 'code',
            '.sh': 'code', '.bat': 'code', '.ps1': 'code', '.sql': 'code',
            
            # Document Files
            '.pdf': 'document', '.docx': 'document', '.doc': 'document',
            '.xlsx': 'document', '.xls': 'document', '.pptx': 'document',
            '.ppt': 'document', '.odt': 'document', '.ods': 'document',
            
            # Data Files
            '.csv': 'data', '.tsv': 'data', '.xml': 'data', '.json': 'data',
            
            # Configuration Files
            '.env': 'config', '.gitignore': 'config', '.dockerfile': 'config',
            '.dockerignore': 'config', 'docker-compose.yml': 'config',
            'package.json': 'config', 'requirements.txt': 'config',
            'pom.xml': 'config', 'build.gradle': 'config', 'Cargo.toml': 'config',
            'go.mod': 'config', 'composer.json': 'config', 'Gemfile': 'config'
        }
        
        # File size limits (in bytes)
        self.size_limits = {
            'text': 10 * 1024 * 1024,  # 10MB
            'code': 5 * 1024 * 1024,   # 5MB
            'document': 50 * 1024 * 1024,  # 50MB
            'data': 100 * 1024 * 1024,  # 100MB
            'config': 1 * 1024 * 1024   # 1MB
        }
        
        # Performance settings
        self.max_workers = self.config.get('max_workers', min(32, (os.cpu_count() or 1) + 4))
        self.chunk_size = self.config.get('chunk_size', 1000)
        self.chunk_overlap = self.config.get('chunk_overlap', 200)
        
        logger.info(f"üöÄ Intelligent File Processor ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Workers: {self.max_workers})")
    
    async def process_directory_deep(self, root_path: str, 
                                   include_patterns: List[str] = None,
                                   exclude_patterns: List[str] = None) -> List[DocumentContent]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∂‡∏Å"""
        start_time = time.time()
        
        try:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á patterns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
            include_patterns = include_patterns or ['*']
            exclude_patterns = exclude_patterns or [
                '*.exe', '*.dll', '*.so', '*.dylib', '*.bin',
                '*.zip', '*.tar', '*.gz', '*.rar', '*.7z',
                '*.mp3', '*.mp4', '*.avi', '*.mov', '*.wmv',
                '*.jpg', '*.jpeg', '*.png', '*.gif', '*.bmp',
                '*.iso', '*.img', '*.vmdk', '*.vhd'
            ]
            
            # ‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            all_files = self._find_files_recursive(root_path, include_patterns, exclude_patterns)
            logger.info(f"üîç ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {len(all_files)} ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô {root_path}")
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö parallel
            documents = await self._process_files_parallel(all_files)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            logger.info(f"‚úÖ ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {len(documents)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡πÉ‡∏ô {processing_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
            logger.info(f"üìä ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡πÄ‡∏£‡πá‡∏ß: {len(documents)/processing_time:.2f} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£/‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
            
            return documents
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {e}")
            return []
    
    def _find_files_recursive(self, root_path: str, include_patterns: List[str], 
                            exclude_patterns: List[str]) -> List[str]:
        """‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö recursive ‡∏û‡∏£‡πâ‡∏≠‡∏° pattern matching"""
        files = []
        root_path = Path(root_path)
        
        try:
            for pattern in include_patterns:
                for file_path in root_path.rglob(pattern):
                    if file_path.is_file():
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö exclude patterns
                        should_exclude = False
                        for exclude_pattern in exclude_patterns:
                            if file_path.match(exclude_pattern):
                                should_exclude = True
                                break
                        
                        if not should_exclude:
                            files.append(str(file_path))
            
            # ‡∏•‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥
            files = list(set(files))
            return files
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå: {e}")
            return []
    
    async def _process_files_parallel(self, file_paths: List[str]) -> List[DocumentContent]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö parallel"""
        documents = []
        
        # ‡πÅ‡∏ö‡πà‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡πá‡∏ô batches
        batch_size = max(1, len(file_paths) // self.max_workers)
        batches = [file_paths[i:i + batch_size] for i in range(0, len(file_paths), batch_size)]
        
        # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏ö‡∏ö parallel
        tasks = []
        for batch in batches:
            task = asyncio.create_task(self._process_file_batch(batch))
            tasks.append(task)
        
        # ‡∏£‡∏≠‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        for batch_result in batch_results:
            if isinstance(batch_result, list):
                documents.extend(batch_result)
            else:
                logger.error(f"‚ùå Batch processing error: {batch_result}")
        
        return documents
    
    async def _process_file_batch(self, file_paths: List[str]) -> List[DocumentContent]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• batch ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        documents = []
        
        for file_path in file_paths:
            try:
                doc = await self._process_single_file(file_path)
                if doc:
                    documents.append(doc)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå {file_path}: {e}")
                continue
        
        return documents
    
    async def _process_single_file(self, file_path: str) -> Optional[DocumentContent]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß"""
        start_time = time.time()
        
        try:
            file_path = Path(file_path)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
            if not file_path.exists():
                return None
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå
            file_size = file_path.stat().st_size
            if file_size == 0:
                return None
            
            # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÑ‡∏ü‡∏•‡πå
            content_type = self._determine_content_type(file_path)
            if not content_type:
                return None
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
            size_limit = self.size_limits.get(content_type, 1 * 1024 * 1024)
            if file_size > size_limit:
                logger.info(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏ü‡∏•‡πå {file_path} (‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ: {file_size} bytes)")
                return None
            
            # ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            content = await self._extract_content(file_path, content_type)
            if not content:
                return None
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á metadata
            metadata = self._create_metadata(file_path, content_type, file_size)
            
            processing_time = time.time() - start_time
            
            return DocumentContent(
                file_path=str(file_path),
                content_type=content_type,
                content=content,
                metadata=metadata,
                extracted_at=datetime.now(),
                file_size=file_size,
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå {file_path}: {e}")
            return None
    
    def _determine_content_type(self, file_path: Path) -> Optional[str]:
        """‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        file_name = file_path.name.lower()
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å extension
        for ext, content_type in self.supported_extensions.items():
            if file_name.endswith(ext):
                return content_type
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        for pattern, content_type in self.supported_extensions.items():
            if pattern in file_name:
                return content_type
        
        # ‡πÉ‡∏ä‡πâ magic number ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
        try:
            mime_type, _ = mimetypes.guess_type(str(file_path))
            if mime_type:
                if mime_type.startswith('text/'):
                    return 'text'
                elif mime_type.startswith('application/'):
                    return 'document'
        except:
            pass
        
        return None
    
    async def _extract_content(self, file_path: Path, content_type: str) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå"""
        try:
            if content_type == 'text':
                return await self._extract_text_content(file_path)
            elif content_type == 'code':
                return await self._extract_code_content(file_path)
            elif content_type == 'document':
                return await self._extract_document_content(file_path)
            elif content_type == 'data':
                return await self._extract_data_content(file_path)
            elif content_type == 'config':
                return await self._extract_config_content(file_path)
            else:
                return None
                
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ {file_path}: {e}")
            return None
    
    async def _extract_text_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"""
        try:
            # ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô UTF-8 ‡∏Å‡πà‡∏≠‡∏ô
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return content
            except UnicodeDecodeError:
                # ‡∏•‡∏≠‡∏á encoding ‡∏≠‡∏∑‡πà‡∏ô‡πÜ
                encodings = ['latin-1', 'cp1252', 'iso-8859-1']
                for encoding in encodings:
                    try:
                        with open(file_path, 'r', encoding=encoding) as f:
                            content = f.read()
                        return content
                    except UnicodeDecodeError:
                        continue
                
                # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô binary ‡πÅ‡∏•‡∏∞ decode ‡πÅ‡∏ö‡∏ö ignore
                with open(file_path, 'rb') as f:
                    content = f.read().decode('utf-8', errors='ignore')
                return content
                
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° {file_path}: {e}")
            return None
    
    async def _extract_code_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÇ‡∏Ñ‡πâ‡∏î"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î
            code_metadata = self._analyze_code_structure(content, file_path.suffix)
            
            # ‡∏£‡∏ß‡∏° metadata ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            enhanced_content = f"// File: {file_path.name}\n"
            enhanced_content += f"// Language: {code_metadata.get('language', 'unknown')}\n"
            enhanced_content += f"// Functions: {len(code_metadata.get('functions', []))}\n"
            enhanced_content += f"// Classes: {len(code_metadata.get('classes', []))}\n"
            enhanced_content += "// Content:\n"
            enhanced_content += content
            
            return enhanced_content
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÇ‡∏Ñ‡πâ‡∏î {file_path}: {e}")
            return None
    
    def _analyze_code_structure(self, content: str, file_extension: str) -> Dict[str, Any]:
        """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î"""
        metadata = {
            'language': self._get_language_from_extension(file_extension),
            'functions': [],
            'classes': [],
            'imports': [],
            'comments': []
        }
        
        try:
            if file_extension == '.py' and AST_AVAILABLE:
                # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Python code
                try:
                    tree = ast.parse(content)
                    for node in ast.walk(tree):
                        if isinstance(node, ast.FunctionDef):
                            metadata['functions'].append(node.name)
                        elif isinstance(node, ast.ClassDef):
                            metadata['classes'].append(node.name)
                        elif isinstance(node, ast.Import):
                            for alias in node.names:
                                metadata['imports'].append(alias.name)
                        elif isinstance(node, ast.ImportFrom):
                            if node.module:
                                metadata['imports'].append(node.module)
                except:
                    pass
            
            # ‡∏´‡∏≤ comments
            comment_patterns = {
                '.py': [r'#.*$', r'""".*?"""', r"'''.*?'''"],
                '.js': [r'//.*$', r'/\*.*?\*/'],
                '.ts': [r'//.*$', r'/\*.*?\*/'],
                '.java': [r'//.*$', r'/\*.*?\*/'],
                '.cpp': [r'//.*$', r'/\*.*?\*/'],
                '.c': [r'//.*$', r'/\*.*?\*/'],
                '.html': [r'<!--.*?-->'],
                '.css': [r'/\*.*?\*/'],
                '.sql': [r'--.*$', r'/\*.*?\*/']
            }
            
            patterns = comment_patterns.get(file_extension, [])
            for pattern in patterns:
                comments = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
                metadata['comments'].extend(comments)
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡πâ‡∏î‡πÑ‡∏î‡πâ: {e}")
        
        return metadata
    
    def _get_language_from_extension(self, extension: str) -> str:
        """‡πÅ‡∏õ‡∏•‡∏á extension ‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏†‡∏≤‡∏©‡∏≤"""
        language_map = {
            '.py': 'Python', '.js': 'JavaScript', '.ts': 'TypeScript',
            '.jsx': 'React JSX', '.tsx': 'React TSX', '.java': 'Java',
            '.cpp': 'C++', '.c': 'C', '.h': 'C Header', '.hpp': 'C++ Header',
            '.cs': 'C#', '.php': 'PHP', '.rb': 'Ruby', '.go': 'Go',
            '.rs': 'Rust', '.swift': 'Swift', '.kt': 'Kotlin',
            '.scala': 'Scala', '.dart': 'Dart', '.html': 'HTML',
            '.css': 'CSS', '.scss': 'SCSS', '.sass': 'Sass',
            '.xml': 'XML', '.json': 'JSON', '.yaml': 'YAML',
            '.yml': 'YAML', '.toml': 'TOML', '.ini': 'INI',
            '.cfg': 'Config', '.conf': 'Config', '.sh': 'Shell',
            '.bat': 'Batch', '.ps1': 'PowerShell', '.sql': 'SQL'
        }
        return language_map.get(extension, 'Unknown')
    
    def _create_metadata(self, file_path: Path, content_type: str, file_size: int) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå"""
        try:
            stat = file_path.stat()
            
            return {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'content_type': content_type,
                'file_size': file_size,
                'file_extension': file_path.suffix.lower(),
                'created_time': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'accessed_time': datetime.fromtimestamp(stat.st_atime).isoformat(),
                'is_hidden': file_path.name.startswith('.'),
                'parent_directory': str(file_path.parent),
                'depth_level': len(file_path.parts) - 1
            }
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {file_path}: {e}")
            return {}
    
    async def _extract_document_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"""
        try:
            extension = file_path.suffix.lower()
            
            if extension == '.pdf' and PDF_AVAILABLE:
                return await self._extract_pdf_content(file_path)
            elif extension == '.docx' and DOCX_AVAILABLE:
                return await self._extract_docx_content(file_path)
            elif extension in ['.xlsx', '.xls'] and EXCEL_AVAILABLE:
                return await self._extract_excel_content(file_path)
            elif extension in ['.html', '.htm'] and HTML_AVAILABLE:
                return await self._extract_html_content(file_path)
            else:
                # Fallback to text extraction
                return await self._extract_text_content(file_path)
                
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ {file_path}: {e}")
            return None
    
    async def _extract_pdf_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å PDF"""
        try:
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                content = await loop.run_in_executor(executor, self._extract_pdf_sync, file_path)
            return content
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ PDF {file_path}: {e}")
            return None
    
    def _extract_pdf_sync(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å PDF (sync version)"""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                content = ""
                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"
                return content
        except Exception as e:
            logger.error(f"‚ùå PDF extraction error: {e}")
            return None
    
    async def _extract_docx_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å DOCX"""
        try:
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                content = await loop.run_in_executor(executor, self._extract_docx_sync, file_path)
            return content
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ DOCX {file_path}: {e}")
            return None
    
    def _extract_docx_sync(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å DOCX (sync version)"""
        try:
            doc = docx.Document(file_path)
            content = ""
            for paragraph in doc.paragraphs:
                content += paragraph.text + "\n"
            return content
        except Exception as e:
            logger.error(f"‚ùå DOCX extraction error: {e}")
            return None
    
    async def _extract_excel_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å Excel"""
        try:
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                content = await loop.run_in_executor(executor, self._extract_excel_sync, file_path)
            return content
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ Excel {file_path}: {e}")
            return None
    
    def _extract_excel_sync(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å Excel (sync version)"""
        try:
            workbook = openpyxl.load_workbook(file_path, data_only=True)
            content = ""
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                content += f"Sheet: {sheet_name}\n"
                for row in sheet.iter_rows(values_only=True):
                    row_content = [str(cell) if cell is not None else "" for cell in row]
                    content += "\t".join(row_content) + "\n"
                content += "\n"
            return content
        except Exception as e:
            logger.error(f"‚ùå Excel extraction error: {e}")
            return None
    
    async def _extract_html_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å HTML"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # ‡πÉ‡∏ä‡πâ BeautifulSoup ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            soup = BeautifulSoup(content, 'html.parser')
            
            # ‡∏•‡∏ö script ‡πÅ‡∏•‡∏∞ style tags
            for script in soup(["script", "style"]):
                script.decompose()
            
            # ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            text = soup.get_text()
            
            # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ HTML {file_path}: {e}")
            return None
    
    async def _extract_data_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"""
        try:
            extension = file_path.suffix.lower()
            
            if extension == '.csv':
                return await self._extract_csv_content(file_path)
            elif extension == '.json':
                return await self._extract_json_content(file_path)
            elif extension == '.xml':
                return await self._extract_xml_content(file_path)
            else:
                return await self._extract_text_content(file_path)
                
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {file_path}: {e}")
            return None
    
    async def _extract_csv_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å CSV"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata
            lines = content.split('\n')
            if lines:
                headers = lines[0].split(',')
                content = f"CSV File: {file_path.name}\n"
                content += f"Columns: {len(headers)}\n"
                content += f"Headers: {', '.join(headers)}\n"
                content += f"Rows: {len(lines) - 1}\n"
                content += "Content:\n" + '\n'.join(lines[:100])  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 100 ‡πÅ‡∏ñ‡∏ß‡πÅ‡∏£‡∏Å
            
            return content
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ CSV {file_path}: {e}")
            return None
    
    async def _extract_json_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å JSON"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # Parse JSON ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏° metadata
            try:
                data = json.loads(content)
                content = f"JSON File: {file_path.name}\n"
                content += f"Type: {type(data).__name__}\n"
                if isinstance(data, dict):
                    content += f"Keys: {', '.join(data.keys())}\n"
                elif isinstance(data, list):
                    content += f"Items: {len(data)}\n"
                content += "Content:\n" + content
            
            except json.JSONDecodeError:
                # ‡∏ñ‡πâ‡∏≤ parse ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÄ‡∏î‡∏¥‡∏°
                pass
            
            return content
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ JSON {file_path}: {e}")
            return None
    
    async def _extract_xml_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å XML"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # ‡πÉ‡∏ä‡πâ BeautifulSoup ‡πÄ‡∏û‡∏∑‡πà‡∏≠ parse XML
            soup = BeautifulSoup(content, 'xml')
            
            # ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            content = f"XML File: {file_path.name}\n"
            content += f"Root Element: {soup.find().name if soup.find() else 'None'}\n"
            content += "Content:\n" + soup.get_text()
            
            return content
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ XML {file_path}: {e}")
            return None
    
    async def _extract_config_content(self, file_path: Path) -> Optional[str]:
        """‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå config"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° metadata ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö config files
            content = f"Config File: {file_path.name}\n"
            content += f"Type: Configuration\n"
            content += "Content:\n" + content
            
            return content
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ config {file_path}: {e}")
            return None

class EnhancedRAGSystem:
    """Enhanced RAG System ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # Initialize components
        self.file_processor = IntelligentFileProcessor(config.get('file_processor', {}))
        
        # Performance monitoring
        self.metrics = PerformanceMetrics(
            start_time=0,
            end_time=0,
            files_processed=0,
            documents_extracted=0,
            embeddings_generated=0,
            cache_hits=0,
            cache_misses=0,
            errors=[]
        )
        
        # Cache system
        self.cache = redis.Redis(
            host=self.config.get("redis_host", "localhost"),
            port=self.config.get("redis_port", 6379),
            db=self.config.get("redis_db", 1),  # ‡πÉ‡∏ä‡πâ DB 1 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö enhanced system
            decode_responses=True
        )
        
        logger.info("üöÄ Enhanced RAG System ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    
    async def scan_directory_deep(self, root_path: str, 
                                include_patterns: List[str] = None,
                                exclude_patterns: List[str] = None,
                                max_depth: int = 10) -> Dict[str, Any]:
        """‡∏™‡πÅ‡∏Å‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∂‡∏Å"""
        start_time = time.time()
        self.metrics.start_time = start_time
        
        try:
            logger.info(f"üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡πÅ‡∏Å‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∂‡∏Å: {root_path}")
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÑ‡∏ü‡∏•‡πå
            documents = await self.file_processor.process_directory_deep(
                root_path, include_patterns, exclude_patterns
            )
            
            # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó metrics
            self.metrics.files_processed = len(documents)
            self.metrics.documents_extracted = len(documents)
            
            # Cache documents
            await self._cache_documents(documents)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
            if self.config.get('generate_embeddings', True):
                await self._generate_embeddings_batch(documents)
            
            end_time = time.time()
            self.metrics.end_time = end_time
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô
            report = self._create_scan_report(documents)
            
            logger.info(f"‚úÖ ‡∏™‡πÅ‡∏Å‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô: {len(documents)} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡πÉ‡∏ô {end_time - start_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
            
            return report
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô: {e}")
            self.metrics.errors.append(str(e))
            return {'error': str(e)}
    
    async def _cache_documents(self, documents: List[DocumentContent]):
        """Cache ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"""
        try:
            for doc in documents:
                cache_key = f"doc:{hashlib.md5(doc.file_path.encode()).hexdigest()}"
                cache_data = {
                    'file_path': doc.file_path,
                    'content_type': doc.content_type,
                    'content': doc.content[:10000],  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏ô‡∏≤‡∏î
                    'metadata': doc.metadata,
                    'extracted_at': doc.extracted_at.isoformat(),
                    'file_size': doc.file_size,
                    'processing_time': doc.processing_time
                }
                
                # Cache 1 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á
                self.cache.setex(cache_key, 3600, json.dumps(cache_data))
            
            logger.info(f"‚úÖ Cache ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ {len(documents)} ‡πÑ‡∏ü‡∏•‡πå")
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ cache: {e}")
    
    async def _generate_embeddings_batch(self, documents: List[DocumentContent]):
        """‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡πÅ‡∏ö‡∏ö batch"""
        try:
            # ‡πÉ‡∏ä‡πâ RAG system ‡πÄ‡∏î‡∏¥‡∏°
            from .rag_system import RAGSystem
            
            rag = RAGSystem(self.config)
            
            for doc in documents:
                if doc.content:
                    success = await rag.add_document(
                        content=doc.content,
                        metadata=doc.metadata,
                        source=doc.file_path
                    )
                    if success:
                        self.metrics.embeddings_generated += 1
            
            logger.info(f"‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings {self.metrics.embeddings_generated} ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings: {e}")
    
    def _create_scan_report(self, documents: List[DocumentContent]) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô"""
        try:
            # ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
            content_types = {}
            file_extensions = {}
            total_size = 0
            
            for doc in documents:
                # Content types
                content_type = doc.content_type
                if content_type not in content_types:
                    content_types[content_type] = 0
                content_types[content_type] += 1
                
                # File extensions
                ext = doc.metadata.get('file_extension', 'unknown')
                if ext not in file_extensions:
                    file_extensions[ext] = 0
                file_extensions[ext] += 1
                
                # Total size
                total_size += doc.file_size
            
            # Performance metrics
            performance = {
                'total_time': self.metrics.total_time,
                'files_per_second': self.metrics.files_per_second,
                'documents_per_second': self.metrics.documents_per_second,
                'embeddings_generated': self.metrics.embeddings_generated,
                'cache_hits': self.metrics.cache_hits,
                'cache_misses': self.metrics.cache_misses
            }
            
            return {
                'summary': {
                    'total_documents': len(documents),
                    'total_size_bytes': total_size,
                    'total_size_mb': total_size / (1024 * 1024),
                    'content_types': content_types,
                    'file_extensions': dict(sorted(file_extensions.items(), key=lambda x: x[1], reverse=True)[:20]),
                    'performance': performance
                },
                'documents': [
                    {
                        'file_path': doc.file_path,
                        'content_type': doc.content_type,
                        'file_size': doc.file_size,
                        'processing_time': doc.processing_time,
                        'metadata': doc.metadata
                    }
                    for doc in documents[:100]  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 100 ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏£‡∏Å
                ],
                'errors': self.metrics.errors
            }
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô: {e}")
            return {'error': str(e)}
    
    async def search_documents(self, query: str, content_types: List[str] = None, 
                             top_k: int = 10) -> List[Dict[str, Any]]:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"""
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö cache
            cache_key = f"search:{hashlib.md5(query.encode()).hexdigest()}"
            cached_result = self.cache.get(cache_key)
            
            if cached_result:
                self.metrics.cache_hits += 1
                logger.info("‚úÖ ‡πÉ‡∏ä‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å cache")
                return json.loads(cached_result)
            
            self.metrics.cache_misses += 1
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô RAG system
            from .rag_system import RAGSystem
            rag = RAGSystem(self.config)
            
            results = await rag.search(query, top_k)
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            search_results = []
            for result in results:
                search_results.append({
                    'file_path': result.document.source,
                    'content': result.document.content[:500],  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ç‡∏ô‡∏≤‡∏î
                    'score': result.score,
                    'relevance': result.relevance,
                    'metadata': result.document.metadata
                })
            
            # Cache ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            self.cache.setex(cache_key, 1800, json.dumps(search_results))  # 30 ‡∏ô‡∏≤‡∏ó‡∏µ
            
            return search_results
            
        except Exception as e:
            logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤: {e}")
            return []
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û"""
        return {
            'total_time': self.metrics.total_time,
            'files_processed': self.metrics.files_processed,
            'documents_extracted': self.metrics.documents_extracted,
            'embeddings_generated': self.metrics.embeddings_generated,
            'files_per_second': self.metrics.files_per_second,
            'documents_per_second': self.metrics.documents_per_second,
            'cache_hits': self.metrics.cache_hits,
            'cache_misses': self.metrics.cache_misses,
            'cache_hit_rate': self.metrics.cache_hits / (self.metrics.cache_hits + self.metrics.cache_misses) if (self.metrics.cache_hits + self.metrics.cache_misses) > 0 else 0,
            'errors': self.metrics.errors
        }
