#!/usr/bin/env python3
"""
Dataset Generator for File System MCP Tool Training.
This script generates a dataset for training an AI agent to understand how
to use the file system management tools.
"""

import json
import random
import sqlite3
from datetime import datetime
from typing import Dict, List, Tuple, Any
from pathlib import Path
import os

class FileSystemDatasetGenerator:
    """
    A dataset generator for training an AI agent.

    This class provides methods for generating a dataset for training an AI
    agent to understand how to use the file system management tools.
    """
    
    def __init__(self, db_path: str = "file_system_analysis.db"):
        """
        Initializes the FileSystemDatasetGenerator.

        Args:
            db_path (str, optional): The path to the database. Defaults to
                                     "file_system_analysis.db".
        """
        self.db_path = db_path
        self.dataset = []
        
    def generate_training_dataset(self, output_file: str = "file_system_training_dataset.json"):
        """
        Generates a training dataset for the AI agent.

        Args:
            output_file (str, optional): The path to the output file.
                                      Defaults to "file_system_training_dataset.json".
        """
        print("üöÄ Starting to generate the training dataset for the AI agent...")
        
        # 1. Generate basic queries
        self._generate_basic_queries()
        
        # 2. Generate file search queries
        self._generate_file_search_queries()
        
        # 3. Generate analysis queries
        self._generate_analysis_queries()
        
        # 4. Generate SQL queries
        self._generate_sql_queries()
        
        # 5. Generate file management queries
        self._generate_file_management_queries()
        
        # 6. Generate reporting queries
        self._generate_reporting_queries()
        
        # 7. Save the dataset
        self._save_dataset(output_file)
        
        print(f"‚úÖ Successfully generated the dataset: {len(self.dataset)} items")
        print(f"üìÅ Saved to: {output_file}")
        
    def _generate_basic_queries(self):
        """Generates a set of basic queries."""
        basic_queries = [
            {
                "instruction": "Summarize this folder for me.",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_directory_summary",
                    "session_id": "scan_xxx",
                    "args": []
                },
                "category": "basic",
                "description": "Request a basic summary of the folder."
            },
            {
                "instruction": "How many files are in this folder in total?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT COUNT(*) as total_files FROM files WHERE session_id = ?",
                    "params": ["scan_xxx"]
                },
                "category": "basic",
                "description": "Count the total number of files."
            },
            {
                "instruction": "What is the total size of the folder?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT SUM(file_size) as total_size FROM files WHERE session_id = ?",
                    "params": ["scan_xxx"]
                },
                "category": "basic",
                "description": "Calculate the total size of the folder."
            },
            {
                "instruction": "Show the 10 largest files.",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_largest_files",
                    "session_id": "scan_xxx",
                    "args": [10]
                },
                "category": "basic",
                "description": "Show the largest files."
            },
            {
                "instruction": "Are there any duplicate files?",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_duplicate_files",
                    "session_id": "scan_xxx",
                    "args": []
                },
                "category": "basic",
                "description": "Check for duplicate files."
            }
        ]
        
        self.dataset.extend(basic_queries)
        
    def _generate_file_search_queries(self):
        """Generates a set of file search queries."""
        file_search_queries = [
            {
                "instruction": "Find the 5 largest Word document files.",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND file_extension = '.docx' ORDER BY file_size DESC LIMIT 5",
                    "params": ["scan_xxx"]
                },
                "category": "file_search",
                "description": "Find large Word files."
            },
            {
                "instruction": "Show all image files.",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND mime_type LIKE 'image/%'",
                    "params": ["scan_xxx"]
                },
                "category": "file_search",
                "description": "Find image files."
            },
            {
                "instruction": "How many PDF files are in the system?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT COUNT(*) as pdf_count FROM files WHERE session_id = ? AND file_extension = '.pdf'",
                    "params": ["scan_xxx"]
                },
                "category": "file_search",
                "description": "Count PDF files."
            },
            {
                "instruction": "Show all files with 'report' in the name that are PDFs.",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_path FROM files WHERE session_id = ? AND file_name LIKE '%report%' AND file_extension = '.pdf'",
                    "params": ["scan_xxx"]
                },
                "category": "file_search",
                "description": "Find PDF files with 'report' in the name."
            },
            {
                "instruction": "Find the largest Python code file.",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND file_extension = '.py' ORDER BY file_size DESC LIMIT 1",
                    "params": ["scan_xxx"]
                },
                "category": "file_search",
                "description": "Find the largest Python file."
            },
            {
                "instruction": "Show all JavaScript files.",
                "correct_action": {
                    "action": "query_function",
                    "function": "find_files_by_extension",
                    "session_id": "scan_xxx",
                    "args": [".js"]
                },
                "category": "file_search",
                "description": "Find JavaScript files."
            },
            {
                "instruction": "Which duplicate files are taking up the most space?",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_duplicate_files",
                    "session_id": "scan_xxx",
                    "args": []
                },
                "category": "file_search",
                "description": "Check for duplicate files that are taking up the most space."
            }
        ]
        
        self.dataset.extend(file_search_queries)
        
    def _generate_analysis_queries(self):
        """Generates a set of analysis queries."""
        analysis_queries = [
            {
                "instruction": "Analyze the file types found in the folder.",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_extension, COUNT(*) as count, SUM(file_size) as total_size FROM files WHERE session_id = ? GROUP BY file_extension ORDER BY count DESC",
                    "params": ["scan_xxx"]
                },
                "category": "analysis",
                "description": "Analyze file types."
            },
            {
                "instruction": "Which file types use the most space?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_extension, SUM(file_size) as total_size FROM files WHERE session_id = ? GROUP BY file_extension ORDER BY total_size DESC LIMIT 5",
                    "params": ["scan_xxx"]
                },
                "category": "analysis",
                "description": "Analyze which files use the most space."
            },
            {
                "instruction": "How many hidden files are there?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT COUNT(*) as hidden_count FROM files WHERE session_id = ? AND is_hidden = 1",
                    "params": ["scan_xxx"]
                },
                "category": "analysis",
                "description": "Count hidden files."
            },
            {
                "instruction": "Which are the oldest files?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, created_date FROM files WHERE session_id = ? ORDER BY created_date ASC LIMIT 10",
                    "params": ["scan_xxx"]
                },
                "category": "analysis",
                "description": "Find the oldest files."
            },
            {
                "instruction": "Which files were modified most recently?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, modified_date FROM files WHERE session_id = ? ORDER BY modified_date DESC LIMIT 10",
                    "params": ["scan_xxx"]
                },
                "category": "analysis",
                "description": "Find the most recently modified files."
            }
        ]
        
        self.dataset.extend(analysis_queries)
        
    def _generate_sql_queries(self):
        """Generates a set of SQL queries."""
        sql_queries = [
            {
                "instruction": "Show files larger than 100MB.",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND file_size > 104857600 ORDER BY file_size DESC",
                    "params": ["scan_xxx"]
                },
                "category": "sql",
                "description": "Find large files."
            },
            {
                "instruction": "What are the image files larger than 10MB?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND mime_type LIKE 'image/%' AND file_size > 10485760",
                    "params": ["scan_xxx"]
                },
                "category": "sql",
                "description": "Find large image files."
            },
            {
                "instruction": "Show files created this month.",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, created_date FROM files WHERE session_id = ? AND created_date >= date('now', 'start of month')",
                    "params": ["scan_xxx"]
                },
                "category": "sql",
                "description": "Find files created this month."
            },
            {
                "instruction": "What files were modified this week?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, modified_date FROM files WHERE session_id = ? AND modified_date >= date('now', '-7 days')",
                    "params": ["scan_xxx"]
                },
                "category": "sql",
                "description": "Find files modified this week."
            },
            {
                "instruction": "Show files with no extension.",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path FROM files WHERE session_id = ? AND (file_extension = '' OR file_extension IS NULL)",
                    "params": ["scan_xxx"]
                },
                "category": "sql",
                "description": "Find files with no extension."
            }
        ]
        
        self.dataset.extend(sql_queries)
        
    def _generate_file_management_queries(self):
        """Generates a set of file management queries."""
        management_queries = [
            {
                "instruction": "Which files should be deleted to save space?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, file_size FROM files WHERE session_id = ? AND file_size > 52428800 ORDER BY file_size DESC LIMIT 20",
                    "params": ["scan_xxx"]
                },
                "category": "management",
                "description": "Find files that should be deleted."
            },
            {
                "instruction": "What temporary files are there?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path FROM files WHERE session_id = ? AND (file_name LIKE '%.tmp' OR file_name LIKE 'temp%' OR file_name LIKE '%cache%')",
                    "params": ["scan_xxx"]
                },
                "category": "management",
                "description": "Find temporary files."
            },
            {
                "instruction": "Which files were accessed most recently?",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path, accessed_date FROM files WHERE session_id = ? ORDER BY accessed_date DESC LIMIT 10",
                    "params": ["scan_xxx"]
                },
                "category": "management",
                "description": "Find the most recently accessed files."
            },
            {
                "instruction": "Show files that might be viruses.",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_name, file_path FROM files WHERE session_id = ? AND file_extension IN ('.exe', '.bat', '.cmd', '.scr', '.pif')",
                    "params": ["scan_xxx"]
                },
                "category": "management",
                "description": "Find files that might be viruses."
            }
        ]
        
        self.dataset.extend(management_queries)
        
    def _generate_reporting_queries(self):
        """Generates a set of reporting queries."""
        reporting_queries = [
            {
                "instruction": "Create a summary report of the folder.",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_directory_summary",
                    "session_id": "scan_xxx",
                    "args": []
                },
                "category": "reporting",
                "description": "Create a summary report."
            },
            {
                "instruction": "Report the 20 largest files.",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_largest_files",
                    "session_id": "scan_xxx",
                    "args": [20]
                },
                "category": "reporting",
                "description": "Report large files."
            },
            {
                "instruction": "Report all duplicate files.",
                "correct_action": {
                    "action": "query_function",
                    "function": "get_duplicate_files",
                    "session_id": "scan_xxx",
                    "args": []
                },
                "category": "reporting",
                "description": "Report duplicate files."
            },
            {
                "instruction": "File type statistics.",
                "correct_action": {
                    "action": "query_sql",
                    "sql": "SELECT file_extension, COUNT(*) as count, AVG(file_size) as avg_size FROM files WHERE session_id = ? GROUP BY file_extension ORDER BY count DESC",
                    "params": ["scan_xxx"]
                },
                "category": "reporting",
                "description": "File type statistics."
            }
        ]
        
        self.dataset.extend(reporting_queries)
        
    def _save_dataset(self, output_file: str):
        """
        Saves the dataset to a file.

        Args:
            output_file (str): The path to the output file.
        """
        dataset_info = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_samples": len(self.dataset),
                "categories": list(set(item["category"] for item in self.dataset)),
                "description": "Dataset for training an AI agent to understand how to use the File System MCP Tool."
            },
            "dataset": self.dataset
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dataset_info, f, ensure_ascii=False, indent=2)
            
    def generate_variations(self, base_dataset_file: str, output_file: str = "expanded_dataset.json"):
        """
        Generates a more diverse dataset by creating variations of the
        instructions in the base dataset.

        Args:
            base_dataset_file (str): The path to the base dataset file.
            output_file (str, optional): The path to the output file.
                                      Defaults to "expanded_dataset.json".
        """
        print("üîÑ Generating a more diverse dataset...")
        
        with open(base_dataset_file, 'r', encoding='utf-8') as f:
            base_data = json.load(f)
            
        expanded_dataset = base_data["dataset"].copy()
        
        # Create a variety of commands
        variations = [
            ("Show large files", ["Show the largest files", "Which files are the largest", "What are the large files"]),
            ("Find duplicate files", ["Are there duplicate files", "Which files are duplicates", "Check for duplicate files"]),
            ("Summarize data", ["Summarize the folder", "Summary data", "Summary report"]),
            ("Image files", ["All image files", "Image file", "What are the images"]),
            ("Document files", ["Word files", "All documents", ".docx files"])
        ]
        
        for original, variations_list in variations:
            for variation in variations_list:
                # Find the entry that matches the original
                for entry in base_data["dataset"]:
                    if original in entry["instruction"]:
                        new_entry = entry.copy()
                        new_entry["instruction"] = entry["instruction"].replace(original, variation)
                        new_entry["category"] = "variation"
                        expanded_dataset.append(new_entry)
                        
        # Save the expanded dataset
        expanded_info = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_samples": len(expanded_dataset),
                "base_samples": len(base_data["dataset"]),
                "variations_added": len(expanded_dataset) - len(base_data["dataset"]),
                "description": "Expanded dataset with variations"
            },
            "dataset": expanded_dataset
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(expanded_info, f, ensure_ascii=False, indent=2)
            
        print(f"‚úÖ Successfully generated the expanded dataset: {len(expanded_dataset)} items")
        
    def generate_test_dataset(self, training_dataset_file: str, output_file: str = "test_dataset.json"):
        """
        Generates a test dataset from the training dataset.

        Args:
            training_dataset_file (str): The path to the training dataset file.
            output_file (str, optional): The path to the output file.
                                      Defaults to "test_dataset.json".
        """
        print("üß™ Generating a test dataset...")
        
        with open(training_dataset_file, 'r', encoding='utf-8') as f:
            training_data = json.load(f)
            
        # Randomly select 20% for testing
        test_samples = random.sample(training_data["dataset"], 
                                   min(len(training_data["dataset"]) // 5, 50))
        
        test_info = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "total_samples": len(test_samples),
                "source": training_dataset_file,
                "description": "Test dataset for File System MCP Tool"
            },
            "dataset": test_samples
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(test_info, f, ensure_ascii=False, indent=2)
            
        print(f"‚úÖ Successfully generated the test dataset: {len(test_samples)} items")
        
    def analyze_dataset(self, dataset_file: str):
        """
        Analyzes a dataset.

        Args:
            dataset_file (str): The path to the dataset file.
        """
        print(f"üìä Analyzing dataset: {dataset_file}")
        
        with open(dataset_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        dataset = data["dataset"]
        
        # Analyze by category
        categories = {}
        for item in dataset:
            cat = item["category"]
            if cat not in categories:
                categories[cat] = 0
            categories[cat] += 1
            
        print("\nüìà Dataset statistics:")
        print(f"‚Ä¢ Total number of samples: {len(dataset)}")
        print(f"‚Ä¢ Number of categories: {len(categories)}")
        
        print("\nüìã Categories:")
        for cat, count in sorted(categories.items()):
            percentage = (count / len(dataset)) * 100
            print(f"  ‚Ä¢ {cat}: {count} ({percentage:.1f}%)")
            
        # Analyze actions
        actions = {}
        for item in dataset:
            action = item["correct_action"]["action"]
            if action not in actions:
                actions[action] = 0
            actions[action] += 1
            
        print("\nüîß Actions used:")
        for action, count in sorted(actions.items()):
            percentage = (count / len(dataset)) * 100
            print(f"  ‚Ä¢ {action}: {count} ({percentage:.1f}%)")

def main():
    """The main function of the script."""
    print("üöÄ File System MCP Dataset Generator")
    print("=" * 50)
    
    generator = FileSystemDatasetGenerator()
    
    # Generate the base dataset
    generator.generate_training_dataset("file_system_training_dataset.json")
    
    # Generate the expanded dataset
    generator.generate_variations("file_system_training_dataset.json", "expanded_dataset.json")
    
    # Generate the test dataset
    generator.generate_test_dataset("file_system_training_dataset.json", "test_dataset.json")
    
    # Analyze the datasets
    print("\n" + "=" * 50)
    generator.analyze_dataset("file_system_training_dataset.json")
    
    print("\n" + "=" * 50)
    generator.analyze_dataset("expanded_dataset.json")
    
    print("\n" + "=" * 50)
    generator.analyze_dataset("test_dataset.json")
    
    print("\nüéâ Dataset generation complete!")
    print("\nüìÅ Files created:")
    print("  ‚Ä¢ file_system_training_dataset.json - Base dataset")
    print("  ‚Ä¢ expanded_dataset.json - Expanded dataset")
    print("  ‚Ä¢ test_dataset.json - Test dataset")

if __name__ == "__main__":
    main()
