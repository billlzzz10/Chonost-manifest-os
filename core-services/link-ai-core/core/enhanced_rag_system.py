"""
ðŸš€ Enhanced RAG System - High Performance & Deep Document Scanning
à¸£à¸°à¸šà¸š RAG à¸—à¸µà¹ˆà¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸žà¹à¸¥à¸°à¸£à¸­à¸‡à¸£à¸±à¸šà¸à¸²à¸£à¸ªà¹à¸à¸™à¹€à¸­à¸à¸ªà¸²à¸£à¸£à¸°à¸”à¸±à¸šà¸¥à¸¶à¸

Features:
- Parallel Document Processing
- Intelligent File Type Detection
- Advanced Caching Strategy
- Performance Monitoring
- Deep Code Analysis
- Document Content Extraction
"""

import asyncio
import json
import logging
import hashlib
import time
from datetime import datetime
from typing import Dict, List, Optional, Any, Union, Tuple, Set
from dataclasses import dataclass, asdict
from pathlib import Path
import numpy as np
import sqlite3
import redis
import requests
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import threading
import multiprocessing
import mimetypes
import magic
import re
import os

# Document Processing Imports
try:
    import PyPDF2
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    import docx
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    import openpyxl
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False

try:
    from bs4 import BeautifulSoup
    HTML_AVAILABLE = True
except ImportError:
    HTML_AVAILABLE = False

# Code Analysis Imports
try:
    import ast
    import tokenize
    AST_AVAILABLE = True
except ImportError:
    AST_AVAILABLE = False

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """à¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸žà¸à¸²à¸£à¸—à¸³à¸‡à¸²à¸™"""
    start_time: float
    end_time: float
    files_processed: int
    documents_extracted: int
    embeddings_generated: int
    cache_hits: int
    cache_misses: int
    errors: List[str]
    
    @property
    def total_time(self) -> float:
        return self.end_time - self.start_time
    
    @property
    def files_per_second(self) -> float:
        return self.files_processed / self.total_time if self.total_time > 0 else 0
    
    @property
    def documents_per_second(self) -> float:
        return self.documents_extracted / self.total_time if self.total_time > 0 else 0

@dataclass
class DocumentContent:
    """à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸‚à¸­à¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¸ªà¸à¸±à¸”à¹„à¸”à¹‰"""
    file_path: str
    content_type: str  # 'text', 'code', 'document', 'image', 'binary'
    content: str
    metadata: Dict[str, Any]
    extracted_at: datetime
    file_size: int
    processing_time: float

class IntelligentFileProcessor:
    """à¸£à¸°à¸šà¸šà¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸Ÿà¸¥à¹Œà¸­à¸±à¸ˆà¸‰à¸£à¸´à¸¢à¸°"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        self.supported_extensions = {
            # Text & Code Files
            '.txt': 'text', '.md': 'text', '.rst': 'text', '.log': 'text',
            '.py': 'code', '.js': 'code', '.ts': 'code', '.jsx': 'code', '.tsx': 'code',
            '.java': 'code', '.cpp': 'code', '.c': 'code', '.h': 'code', '.hpp': 'code',
            '.cs': 'code', '.php': 'code', '.rb': 'code', '.go': 'code', '.rs': 'code',
            '.swift': 'code', '.kt': 'code', '.scala': 'code', '.dart': 'code',
            '.html': 'code', '.css': 'code', '.scss': 'code', '.sass': 'code',
            '.xml': 'code', '.json': 'code', '.yaml': 'code', '.yml': 'code',
            '.toml': 'code', '.ini': 'code', '.cfg': 'code', '.conf': 'code',
            '.sh': 'code', '.bat': 'code', '.ps1': 'code', '.sql': 'code',
            
            # Document Files
            '.pdf': 'document', '.docx': 'document', '.doc': 'document',
            '.xlsx': 'document', '.xls': 'document', '.pptx': 'document',
            '.ppt': 'document', '.odt': 'document', '.ods': 'document',
            
            # Data Files
            '.csv': 'data', '.tsv': 'data', '.xml': 'data', '.json': 'data',
            
            # Configuration Files
            '.env': 'config', '.gitignore': 'config', '.dockerfile': 'config',
            '.dockerignore': 'config', 'docker-compose.yml': 'config',
            'package.json': 'config', 'requirements.txt': 'config',
            'pom.xml': 'config', 'build.gradle': 'config', 'Cargo.toml': 'config',
            'go.mod': 'config', 'composer.json': 'config', 'Gemfile': 'config'
        }
        
        # File size limits (in bytes)
        self.size_limits = {
            'text': 10 * 1024 * 1024,  # 10MB
            'code': 5 * 1024 * 1024,   # 5MB
            'document': 50 * 1024 * 1024,  # 50MB
            'data': 100 * 1024 * 1024,  # 100MB
            'config': 1 * 1024 * 1024   # 1MB
        }
        
        # Performance settings
        self.max_workers = self.config.get('max_workers', min(32, (os.cpu_count() or 1) + 4))
        self.chunk_size = self.config.get('chunk_size', 1000)
        self.chunk_overlap = self.config.get('chunk_overlap', 200)
        
        logger.info(f"ðŸš€ Intelligent File Processor à¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ (Workers: {self.max_workers})")
    
    async def process_directory_deep(self, root_path: str, 
                                   include_patterns: List[str] = None,
                                   exclude_patterns: List[str] = None) -> List[DocumentContent]:
        """à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¹à¸šà¸šà¸¥à¸¶à¸"""
        start_time = time.time()
        
        try:
            # à¸ªà¸£à¹‰à¸²à¸‡ patterns à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸à¸£à¸­à¸‡
            include_patterns = include_patterns or ['*']
            exclude_patterns = exclude_patterns or [
                '*.exe', '*.dll', '*.so', '*.dylib', '*.bin',
                '*.zip', '*.tar', '*.gz', '*.rar', '*.7z',
                '*.mp3', '*.mp4', '*.avi', '*.mov', '*.wmv',
                '*.jpg', '*.jpeg', '*.png', '*.gif', '*.bmp',
                '*.iso', '*.img', '*.vmdk', '*.vhd'
            ]
            
            # à¸«à¸²à¹„à¸Ÿà¸¥à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
            all_files = self._find_files_recursive(root_path, include_patterns, exclude_patterns)
            logger.info(f"ðŸ” à¸žà¸šà¹„à¸Ÿà¸¥à¹Œ {len(all_files)} à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸™ {root_path}")
            
            # à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹à¸šà¸š parallel
            documents = await self._process_files_parallel(all_files)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            logger.info(f"âœ… à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™: {len(documents)} à¹€à¸­à¸à¸ªà¸²à¸£ à¹ƒà¸™ {processing_time:.2f} à¸§à¸´à¸™à¸²à¸—à¸µ")
            logger.info(f"ðŸ“Š à¸­à¸±à¸•à¸£à¸²à¹€à¸£à¹‡à¸§: {len(documents)/processing_time:.2f} à¹€à¸­à¸à¸ªà¸²à¸£/à¸§à¸´à¸™à¸²à¸—à¸µ")
            
            return documents
            
        except Exception as e:
            logger.error(f"âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ: {e}")
            return []
    
    def _find_files_recursive(self, root_path: str, include_patterns: List[str], 
                            exclude_patterns: List[str]) -> List[str]:
        """à¸«à¸²à¹„à¸Ÿà¸¥à¹Œà¹à¸šà¸š recursive à¸žà¸£à¹‰à¸­à¸¡ pattern matching"""
        files = []
        root_path = Path(root_path)
        
        try:
            for pattern in include_patterns:
                for file_path in root_path.rglob(pattern):
                    if file_path.is_file():
                        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š exclude patterns
                        should_exclude = False
                        for exclude_pattern in exclude_patterns:
                            if file_path.match(exclude_pattern):
                                should_exclude = True
                                break
                        
                        if not should_exclude:
                            files.append(str(file_path))
            
            # à¸¥à¸šà¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¸‹à¹‰à¸³
            files = list(set(files))
            return files
            
        except Exception as e:
            logger.error(f"âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸«à¸²à¹„à¸Ÿà¸¥à¹Œ: {e}")
            return []
    
    async def _process_files_parallel(self, file_paths: List[str]) -> List[DocumentContent]:
        """à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸Ÿà¸¥à¹Œà¹à¸šà¸š parallel"""
        documents = []
        
        # à¹à¸šà¹ˆà¸‡à¹„à¸Ÿà¸¥à¹Œà¹€à¸›à¹‡à¸™ batches
        batch_size = max(1, len(file_paths) // self.max_workers)
        batches = [file_paths[i:i + batch_size] for i in range(0, len(file_paths), batch_size)]
        
        # à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹à¸šà¸š parallel
        tasks = []
        for batch in batches:
            task = asyncio.create_task(self._process_file_batch(batch))
            tasks.append(task)
        
        # à¸£à¸­à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # à¸£à¸§à¸¡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ
        for batch_result in batch_results:
            if isinstance(batch_result, list):
                documents.extend(batch_result)
            else:
                logger.error(f"âŒ Batch processing error: {batch_result}")
        
        return documents
    
    async def _process_file_batch(self, file_paths: List[str]) -> List[DocumentContent]:
        """à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ batch à¸‚à¸­à¸‡à¹„à¸Ÿà¸¥à¹Œ"""
        documents = []
        
        for file_path in file_paths:
            try:
                doc = await self._process_single_file(file_path)
                if doc:
                    documents.append(doc)
            except Exception as e:
                logger.warning(f"âš ï¸ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸Ÿà¸¥à¹Œ {file_path}: {e}")
                continue
        
        return documents
    
    async def _process_single_file(self, file_path: str) -> Optional[DocumentContent]:
        """à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸Ÿà¸¥à¹Œà¹€à¸”à¸µà¸¢à¸§"""
        start_time = time.time()
        
        try:
            file_path = Path(file_path)
            
            # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹„à¸Ÿà¸¥à¹Œà¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¸ˆà¸£à¸´à¸‡
            if not file_path.exists():
                return None
            
            # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¸™à¸²à¸”à¹„à¸Ÿà¸¥à¹Œ
            file_size = file_path.stat().st_size
            if file_size == 0:
                return None
            
            # à¸à¸³à¸«à¸™à¸”à¸›à¸£à¸°à¹€à¸ à¸—à¹„à¸Ÿà¸¥à¹Œ
            content_type = self._determine_content_type(file_path)
            if not content_type:
                return None
            
            # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸‚à¸™à¸²à¸”à¹„à¸Ÿà¸¥à¹Œà¸•à¸²à¸¡à¸›à¸£à¸°à¹€à¸ à¸—
            size_limit = self.size_limits.get(content_type, 1 * 1024 * 1024)
            if file_size > size_limit:
                logger.info(f"âš ï¸ à¸‚à¹‰à¸²à¸¡à¹„à¸Ÿà¸¥à¹Œ {file_path} (à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆà¹€à¸à¸´à¸™à¹„à¸›: {file_size} bytes)")
                return None
            
            # à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²
            content = await self._extract_content(file_path, content_type)
            if not content:
                return None
            
            # à¸ªà¸£à¹‰à¸²à¸‡ metadata
            metadata = self._create_metadata(file_path, content_type, file_size)
            
            processing_time = time.time() - start_time
            
            return DocumentContent(
                file_path=str(file_path),
                content_type=content_type,
                content=content,
                metadata=metadata,
                extracted_at=datetime.now(),
                file_size=file_size,
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸Ÿà¸¥à¹Œ {file_path}: {e}")
            return None
    
    def _determine_content_type(self, file_path: Path) -> Optional[str]:
        """à¸à¸³à¸«à¸™à¸”à¸›à¸£à¸°à¹€à¸ à¸—à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸‚à¸­à¸‡à¹„à¸Ÿà¸¥à¹Œ"""
        file_name = file_path.name.lower()
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ˆà¸²à¸ extension
        for ext, content_type in self.supported_extensions.items():
            if file_name.endswith(ext):
                return content_type
        
        # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸ˆà¸²à¸à¸Šà¸·à¹ˆà¸­à¹„à¸Ÿà¸¥à¹Œ
        for pattern, content_type in self.supported_extensions.items():
            if pattern in file_name:
                return content_type
        
        # à¹ƒà¸Šà¹‰ magic number à¸ªà¸³à¸«à¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œà¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸ˆà¸±à¸
        try:
            mime_type, _ = mimetypes.guess_type(str(file_path))
            if mime_type:
                if mime_type.startswith('text/'):
                    return 'text'
                elif mime_type.startswith('application/'):
                    return 'document'
        except:
            pass
        
        return None
    
    async def _extract_content(self, file_path: Path, content_type: str) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œ"""
        try:
            if content_type == 'text':
                return await self._extract_text_content(file_path)
            elif content_type == 'code':
                return await self._extract_code_content(file_path)
            elif content_type == 'document':
                return await self._extract_document_content(file_path)
            elif content_type == 'data':
                return await self._extract_data_content(file_path)
            elif content_type == 'config':
                return await self._extract_config_content(file_path)
            else:
                return None
                
        except Exception as e:
            logger.error(f"âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸² {file_path}: {e}")
            return None
    
    async def _extract_text_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡"""
        try:
            # à¸¥à¸­à¸‡à¸­à¹ˆà¸²à¸™à¹€à¸›à¹‡à¸™ UTF-8 à¸à¹ˆà¸­à¸™
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return content
            except UnicodeDecodeError:
                # à¸¥à¸­à¸‡ encoding à¸­à¸·à¹ˆà¸™à¹†
                encodings = ['latin-1', 'cp1252', 'iso-8859-1']
                for encoding in encodings:
                    try:
                        with open(file_path, 'r', encoding=encoding) as f:
                            content = f.read()
                        return content
                    except UnicodeDecodeError:
                        continue
                
                # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¹€à¸¥à¸¢ à¹ƒà¸«à¹‰à¸­à¹ˆà¸²à¸™à¹€à¸›à¹‡à¸™ binary à¹à¸¥à¸° decode à¹à¸šà¸š ignore
                with open(file_path, 'rb') as f:
                    content = f.read().decode('utf-8', errors='ignore')
                return content
                
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡ {file_path}: {e}")
            return None
    
    async def _extract_code_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¹‚à¸„à¹‰à¸”"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # à¹€à¸žà¸´à¹ˆà¸¡ metadata à¸ªà¸³à¸«à¸£à¸±à¸šà¹‚à¸„à¹‰à¸”
            code_metadata = self._analyze_code_structure(content, file_path.suffix)
            
            # à¸£à¸§à¸¡ metadata à¹€à¸‚à¹‰à¸²à¸à¸±à¸šà¹€à¸™à¸·à¹‰à¸­à¸«à¸²
            enhanced_content = f"// File: {file_path.name}\n"
            enhanced_content += f"// Language: {code_metadata.get('language', 'unknown')}\n"
            enhanced_content += f"// Functions: {len(code_metadata.get('functions', []))}\n"
            enhanced_content += f"// Classes: {len(code_metadata.get('classes', []))}\n"
            enhanced_content += "// Content:\n"
            enhanced_content += content
            
            return enhanced_content
            
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¹‚à¸„à¹‰à¸” {file_path}: {e}")
            return None
    
    def _analyze_code_structure(self, content: str, file_extension: str) -> Dict[str, Any]:
        """à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸„à¹‰à¸”"""
        metadata = {
            'language': self._get_language_from_extension(file_extension),
            'functions': [],
            'classes': [],
            'imports': [],
            'comments': []
        }
        
        try:
            if file_extension == '.py' and AST_AVAILABLE:
                # à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ Python code
                try:
                    tree = ast.parse(content)
                    for node in ast.walk(tree):
                        if isinstance(node, ast.FunctionDef):
                            metadata['functions'].append(node.name)
                        elif isinstance(node, ast.ClassDef):
                            metadata['classes'].append(node.name)
                        elif isinstance(node, ast.Import):
                            for alias in node.names:
                                metadata['imports'].append(alias.name)
                        elif isinstance(node, ast.ImportFrom):
                            if node.module:
                                metadata['imports'].append(node.module)
                except:
                    pass
            
            # à¸«à¸² comments
            comment_patterns = {
                '.py': [r'#.*$', r'""".*?"""', r"'''.*?'''"],
                '.js': [r'//.*$', r'/\*.*?\*/'],
                '.ts': [r'//.*$', r'/\*.*?\*/'],
                '.java': [r'//.*$', r'/\*.*?\*/'],
                '.cpp': [r'//.*$', r'/\*.*?\*/'],
                '.c': [r'//.*$', r'/\*.*?\*/'],
                '.html': [r'<!--.*?-->'],
                '.css': [r'/\*.*?\*/'],
                '.sql': [r'--.*$', r'/\*.*?\*/']
            }
            
            patterns = comment_patterns.get(file_extension, [])
            for pattern in patterns:
                comments = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
                metadata['comments'].extend(comments)
            
        except Exception as e:
            logger.warning(f"âš ï¸ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡à¹‚à¸„à¹‰à¸”à¹„à¸”à¹‰: {e}")
        
        return metadata
    
    def _get_language_from_extension(self, extension: str) -> str:
        """à¹à¸›à¸¥à¸‡ extension à¹€à¸›à¹‡à¸™à¸Šà¸·à¹ˆà¸­à¸ à¸²à¸©à¸²"""
        language_map = {
            '.py': 'Python', '.js': 'JavaScript', '.ts': 'TypeScript',
            '.jsx': 'React JSX', '.tsx': 'React TSX', '.java': 'Java',
            '.cpp': 'C++', '.c': 'C', '.h': 'C Header', '.hpp': 'C++ Header',
            '.cs': 'C#', '.php': 'PHP', '.rb': 'Ruby', '.go': 'Go',
            '.rs': 'Rust', '.swift': 'Swift', '.kt': 'Kotlin',
            '.scala': 'Scala', '.dart': 'Dart', '.html': 'HTML',
            '.css': 'CSS', '.scss': 'SCSS', '.sass': 'Sass',
            '.xml': 'XML', '.json': 'JSON', '.yaml': 'YAML',
            '.yml': 'YAML', '.toml': 'TOML', '.ini': 'INI',
            '.cfg': 'Config', '.conf': 'Config', '.sh': 'Shell',
            '.bat': 'Batch', '.ps1': 'PowerShell', '.sql': 'SQL'
        }
        return language_map.get(extension, 'Unknown')
    
    def _create_metadata(self, file_path: Path, content_type: str, file_size: int) -> Dict[str, Any]:
        """à¸ªà¸£à¹‰à¸²à¸‡ metadata à¸ªà¸³à¸«à¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œ"""
        try:
            stat = file_path.stat()
            
            return {
                'file_name': file_path.name,
                'file_path': str(file_path),
                'content_type': content_type,
                'file_size': file_size,
                'file_extension': file_path.suffix.lower(),
                'created_time': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                'modified_time': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'accessed_time': datetime.fromtimestamp(stat.st_atime).isoformat(),
                'is_hidden': file_path.name.startswith('.'),
                'parent_directory': str(file_path.parent),
                'depth_level': len(file_path.parts) - 1
            }
            
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸£à¹‰à¸²à¸‡ metadata à¸ªà¸³à¸«à¸£à¸±à¸š {file_path}: {e}")
            return {}
    
    async def _extract_document_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸à¹€à¸­à¸à¸ªà¸²à¸£"""
        try:
            extension = file_path.suffix.lower()
            
            if extension == '.pdf' and PDF_AVAILABLE:
                return await self._extract_pdf_content(file_path)
            elif extension == '.docx' and DOCX_AVAILABLE:
                return await self._extract_docx_content(file_path)
            elif extension in ['.xlsx', '.xls'] and EXCEL_AVAILABLE:
                return await self._extract_excel_content(file_path)
            elif extension in ['.html', '.htm'] and HTML_AVAILABLE:
                return await self._extract_html_content(file_path)
            else:
                # Fallback to text extraction
                return await self._extract_text_content(file_path)
                
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¹€à¸­à¸à¸ªà¸²à¸£ {file_path}: {e}")
            return None
    
    async def _extract_pdf_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸ PDF"""
        try:
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                content = await loop.run_in_executor(executor, self._extract_pdf_sync, file_path)
            return content
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸² PDF {file_path}: {e}")
            return None
    
    def _extract_pdf_sync(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸ PDF (sync version)"""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                content = ""
                for page in pdf_reader.pages:
                    content += page.extract_text() + "\n"
                return content
        except Exception as e:
            logger.error(f"âŒ PDF extraction error: {e}")
            return None
    
    async def _extract_docx_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸ DOCX"""
        try:
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                content = await loop.run_in_executor(executor, self._extract_docx_sync, file_path)
            return content
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸² DOCX {file_path}: {e}")
            return None
    
    def _extract_docx_sync(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸ DOCX (sync version)"""
        try:
            doc = docx.Document(file_path)
            content = ""
            for paragraph in doc.paragraphs:
                content += paragraph.text + "\n"
            return content
        except Exception as e:
            logger.error(f"âŒ DOCX extraction error: {e}")
            return None
    
    async def _extract_excel_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸ Excel"""
        try:
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                content = await loop.run_in_executor(executor, self._extract_excel_sync, file_path)
            return content
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸² Excel {file_path}: {e}")
            return None
    
    def _extract_excel_sync(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸ Excel (sync version)"""
        try:
            workbook = openpyxl.load_workbook(file_path, data_only=True)
            content = ""
            for sheet_name in workbook.sheetnames:
                sheet = workbook[sheet_name]
                content += f"Sheet: {sheet_name}\n"
                for row in sheet.iter_rows(values_only=True):
                    row_content = [str(cell) if cell is not None else "" for cell in row]
                    content += "\t".join(row_content) + "\n"
                content += "\n"
            return content
        except Exception as e:
            logger.error(f"âŒ Excel extraction error: {e}")
            return None
    
    async def _extract_html_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸ HTML"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # à¹ƒà¸Šà¹‰ BeautifulSoup à¹€à¸žà¸·à¹ˆà¸­à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡
            soup = BeautifulSoup(content, 'html.parser')
            
            # à¸¥à¸š script à¹à¸¥à¸° style tags
            for script in soup(["script", "style"]):
                script.decompose()
            
            # à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡
            text = soup.get_text()
            
            # à¸—à¸³à¸„à¸§à¸²à¸¡à¸ªà¸°à¸­à¸²à¸”à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text
            
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸² HTML {file_path}: {e}")
            return None
    
    async def _extract_data_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥"""
        try:
            extension = file_path.suffix.lower()
            
            if extension == '.csv':
                return await self._extract_csv_content(file_path)
            elif extension == '.json':
                return await self._extract_json_content(file_path)
            elif extension == '.xml':
                return await self._extract_xml_content(file_path)
            else:
                return await self._extract_text_content(file_path)
                
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ {file_path}: {e}")
            return None
    
    async def _extract_csv_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸ CSV"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # à¹€à¸žà¸´à¹ˆà¸¡ metadata
            lines = content.split('\n')
            if lines:
                headers = lines[0].split(',')
                content = f"CSV File: {file_path.name}\n"
                content += f"Columns: {len(headers)}\n"
                content += f"Headers: {', '.join(headers)}\n"
                content += f"Rows: {len(lines) - 1}\n"
                content += "Content:\n" + '\n'.join(lines[:100])  # à¸ˆà¸³à¸à¸±à¸” 100 à¹à¸–à¸§à¹à¸£à¸
            
            return content
            
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸² CSV {file_path}: {e}")
            return None
    
    async def _extract_json_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸ JSON"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # Parse JSON à¹€à¸žà¸·à¹ˆà¸­à¹€à¸žà¸´à¹ˆà¸¡ metadata
            try:
                data = json.loads(content)
                content = f"JSON File: {file_path.name}\n"
                content += f"Type: {type(data).__name__}\n"
                if isinstance(data, dict):
                    content += f"Keys: {', '.join(data.keys())}\n"
                elif isinstance(data, list):
                    content += f"Items: {len(data)}\n"
                content += "Content:\n" + content
            
            except json.JSONDecodeError:
                # à¸–à¹‰à¸² parse à¹„à¸¡à¹ˆà¹„à¸”à¹‰ à¹ƒà¸«à¹‰à¹ƒà¸Šà¹‰à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¹€à¸”à¸´à¸¡
                pass
            
            return content
            
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸² JSON {file_path}: {e}")
            return None
    
    async def _extract_xml_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸ XML"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # à¹ƒà¸Šà¹‰ BeautifulSoup à¹€à¸žà¸·à¹ˆà¸­ parse XML
            soup = BeautifulSoup(content, 'xml')
            
            # à¸ªà¸à¸±à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ªà¸³à¸„à¸±à¸
            content = f"XML File: {file_path.name}\n"
            content += f"Root Element: {soup.find().name if soup.find() else 'None'}\n"
            content += "Content:\n" + soup.get_text()
            
            return content
            
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸² XML {file_path}: {e}")
            return None
    
    async def _extract_config_content(self, file_path: Path) -> Optional[str]:
        """à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œ config"""
        try:
            content = await self._extract_text_content(file_path)
            if not content:
                return None
            
            # à¹€à¸žà¸´à¹ˆà¸¡ metadata à¸ªà¸³à¸«à¸£à¸±à¸š config files
            content = f"Config File: {file_path.name}\n"
            content += f"Type: Configuration\n"
            content += "Content:\n" + content
            
            return content
            
        except Exception as e:
            logger.error(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸à¸±à¸”à¹€à¸™à¸·à¹‰à¸­à¸«à¸² config {file_path}: {e}")
            return None

class EnhancedRAGSystem:
    """Enhanced RAG System à¸—à¸µà¹ˆà¸¡à¸µà¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸žà¸ªà¸¹à¸‡"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # Initialize components
        self.file_processor = IntelligentFileProcessor(config.get('file_processor', {}))
        
        # Performance monitoring
        self.metrics = PerformanceMetrics(
            start_time=0,
            end_time=0,
            files_processed=0,
            documents_extracted=0,
            embeddings_generated=0,
            cache_hits=0,
            cache_misses=0,
            errors=[]
        )
        
        # Cache system
        self.cache = redis.Redis(
            host=self.config.get("redis_host", "localhost"),
            port=self.config.get("redis_port", 6379),
            db=self.config.get("redis_db", 1),  # à¹ƒà¸Šà¹‰ DB 1 à¸ªà¸³à¸«à¸£à¸±à¸š enhanced system
            decode_responses=True
        )
        
        logger.info("ðŸš€ Enhanced RAG System à¸žà¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™")
    
    async def scan_directory_deep(self, root_path: str, 
                                include_patterns: List[str] = None,
                                exclude_patterns: List[str] = None,
                                max_depth: int = 10) -> Dict[str, Any]:
        """à¸ªà¹à¸à¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¹à¸šà¸šà¸¥à¸¶à¸"""
        start_time = time.time()
        self.metrics.start_time = start_time
        
        try:
            logger.info(f"ðŸ” à¹€à¸£à¸´à¹ˆà¸¡à¸ªà¹à¸à¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œà¹à¸šà¸šà¸¥à¸¶à¸: {root_path}")
            
            # à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸Ÿà¸¥à¹Œ
            documents = await self.file_processor.process_directory_deep(
                root_path, include_patterns, exclude_patterns
            )
            
            # à¸­à¸±à¸žà¹€à¸”à¸— metrics
            self.metrics.files_processed = len(documents)
            self.metrics.documents_extracted = len(documents)
            
            # Cache documents
            await self._cache_documents(documents)
            
            # à¸ªà¸£à¹‰à¸²à¸‡ embeddings (à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£)
            if self.config.get('generate_embeddings', True):
                await self._generate_embeddings_batch(documents)
            
            end_time = time.time()
            self.metrics.end_time = end_time
            
            # à¸ªà¸£à¹‰à¸²à¸‡à¸£à¸²à¸¢à¸‡à¸²à¸™
            report = self._create_scan_report(documents)
            
            logger.info(f"âœ… à¸ªà¹à¸à¸™à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™: {len(documents)} à¹€à¸­à¸à¸ªà¸²à¸£ à¹ƒà¸™ {end_time - start_time:.2f} à¸§à¸´à¸™à¸²à¸—à¸µ")
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸ªà¹à¸à¸™: {e}")
            self.metrics.errors.append(str(e))
            return {'error': str(e)}
    
    async def _cache_documents(self, documents: List[DocumentContent]):
        """Cache à¹€à¸­à¸à¸ªà¸²à¸£"""
        try:
            for doc in documents:
                cache_key = f"doc:{hashlib.md5(doc.file_path.encode()).hexdigest()}"
                cache_data = {
                    'file_path': doc.file_path,
                    'content_type': doc.content_type,
                    'content': doc.content[:10000],  # à¸ˆà¸³à¸à¸±à¸”à¸‚à¸™à¸²à¸”
                    'metadata': doc.metadata,
                    'extracted_at': doc.extracted_at.isoformat(),
                    'file_size': doc.file_size,
                    'processing_time': doc.processing_time
                }
                
                # Cache 1 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡
                self.cache.setex(cache_key, 3600, json.dumps(cache_data))
            
            logger.info(f"âœ… Cache à¹€à¸­à¸à¸ªà¸²à¸£ {len(documents)} à¹„à¸Ÿà¸¥à¹Œ")
            
        except Exception as e:
            logger.error(f"âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£ cache: {e}")
    
    async def _generate_embeddings_batch(self, documents: List[DocumentContent]):
        """à¸ªà¸£à¹‰à¸²à¸‡ embeddings à¹à¸šà¸š batch"""
        try:
            # à¹ƒà¸Šà¹‰ RAG system à¹€à¸”à¸´à¸¡
            from .rag_system import RAGSystem
            
            rag = RAGSystem(self.config)
            
            for doc in documents:
                if doc.content:
                    success = await rag.add_document(
                        content=doc.content,
                        metadata=doc.metadata,
                        source=doc.file_path
                    )
                    if success:
                        self.metrics.embeddings_generated += 1
            
            logger.info(f"âœ… à¸ªà¸£à¹‰à¸²à¸‡ embeddings {self.metrics.embeddings_generated} à¹€à¸­à¸à¸ªà¸²à¸£")
            
        except Exception as e:
            logger.error(f"âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡ embeddings: {e}")
    
    def _create_scan_report(self, documents: List[DocumentContent]) -> Dict[str, Any]:
        """à¸ªà¸£à¹‰à¸²à¸‡à¸£à¸²à¸¢à¸‡à¸²à¸™à¸à¸²à¸£à¸ªà¹à¸à¸™"""
        try:
            # à¸ˆà¸±à¸”à¸à¸¥à¸¸à¹ˆà¸¡à¸•à¸²à¸¡à¸›à¸£à¸°à¹€à¸ à¸—à¹€à¸™à¸·à¹‰à¸­à¸«à¸²
            content_types = {}
            file_extensions = {}
            total_size = 0
            
            for doc in documents:
                # Content types
                content_type = doc.content_type
                if content_type not in content_types:
                    content_types[content_type] = 0
                content_types[content_type] += 1
                
                # File extensions
                ext = doc.metadata.get('file_extension', 'unknown')
                if ext not in file_extensions:
                    file_extensions[ext] = 0
                file_extensions[ext] += 1
                
                # Total size
                total_size += doc.file_size
            
            # Performance metrics
            performance = {
                'total_time': self.metrics.total_time,
                'files_per_second': self.metrics.files_per_second,
                'documents_per_second': self.metrics.documents_per_second,
                'embeddings_generated': self.metrics.embeddings_generated,
                'cache_hits': self.metrics.cache_hits,
                'cache_misses': self.metrics.cache_misses
            }
            
            return {
                'summary': {
                    'total_documents': len(documents),
                    'total_size_bytes': total_size,
                    'total_size_mb': total_size / (1024 * 1024),
                    'content_types': content_types,
                    'file_extensions': dict(sorted(file_extensions.items(), key=lambda x: x[1], reverse=True)[:20]),
                    'performance': performance
                },
                'documents': [
                    {
                        'file_path': doc.file_path,
                        'content_type': doc.content_type,
                        'file_size': doc.file_size,
                        'processing_time': doc.processing_time,
                        'metadata': doc.metadata
                    }
                    for doc in documents[:100]  # à¸ˆà¸³à¸à¸±à¸” 100 à¹€à¸­à¸à¸ªà¸²à¸£à¹à¸£à¸
                ],
                'errors': self.metrics.errors
            }
            
        except Exception as e:
            logger.error(f"âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡à¸£à¸²à¸¢à¸‡à¸²à¸™: {e}")
            return {'error': str(e)}
    
    async def search_documents(self, query: str, content_types: List[str] = None, 
                             top_k: int = 10) -> List[Dict[str, Any]]:
        """à¸„à¹‰à¸™à¸«à¸²à¹€à¸­à¸à¸ªà¸²à¸£"""
        try:
            # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š cache
            cache_key = f"search:{hashlib.md5(query.encode()).hexdigest()}"
            cached_result = self.cache.get(cache_key)
            
            if cached_result:
                self.metrics.cache_hits += 1
                logger.info("âœ… à¹ƒà¸Šà¹‰à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œà¸ˆà¸²à¸ cache")
                return json.loads(cached_result)
            
            self.metrics.cache_misses += 1
            
            # à¸„à¹‰à¸™à¸«à¸²à¹ƒà¸™ RAG system
            from .rag_system import RAGSystem
            rag = RAGSystem(self.config)
            
            results = await rag.search(query, top_k)
            
            # à¹à¸›à¸¥à¸‡à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ
            search_results = []
            for result in results:
                search_results.append({
                    'file_path': result.document.source,
                    'content': result.document.content[:500],  # à¸ˆà¸³à¸à¸±à¸”à¸‚à¸™à¸²à¸”
                    'score': result.score,
                    'relevance': result.relevance,
                    'metadata': result.document.metadata
                })
            
            # Cache à¸œà¸¥à¸¥à¸±à¸žà¸˜à¹Œ
            self.cache.setex(cache_key, 1800, json.dumps(search_results))  # 30 à¸™à¸²à¸—à¸µ
            
            return search_results
            
        except Exception as e:
            logger.error(f"âŒ à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¸„à¹‰à¸™à¸«à¸²: {e}")
            return []
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸ž"""
        return {
            'total_time': self.metrics.total_time,
            'files_processed': self.metrics.files_processed,
            'documents_extracted': self.metrics.documents_extracted,
            'embeddings_generated': self.metrics.embeddings_generated,
            'files_per_second': self.metrics.files_per_second,
            'documents_per_second': self.metrics.documents_per_second,
            'cache_hits': self.metrics.cache_hits,
            'cache_misses': self.metrics.cache_misses,
            'cache_hit_rate': self.metrics.cache_hits / (self.metrics.cache_hits + self.metrics.cache_misses) if (self.metrics.cache_hits + self.metrics.cache_misses) > 0 else 0,
            'errors': self.metrics.errors
        }
